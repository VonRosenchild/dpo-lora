MILLION = 1.0e6
BILLION = 1.0e9
MODEL_REGISTRY = {
    # causal lms
    "Pythia-70M": {
        "name": "Pythia-70M",
        "type": "CausalLM",
        "family": "GPTNeoX",
        "path": "EleutherAI/pythia-70m",
        "sequence_length": 2048,
        "n_params": 70 * MILLION,
        "n_layers": 6,
        "fast_tokenizer": True,
    },
    "Pythia-160M": {
        "name": "Pythia-160M",
        "type": "CausalLM",
        "family": "GPTNeoX",
        "path": "EleutherAI/pythia-160m",
        "sequence_length": 2048,
        "n_params": 160 * MILLION,
        "n_layers": 12,
        "fast_tokenizer": True,
    },
    "Pythia-410M": {
        "name": "Pythia-410M",
        "type": "CausalLM",
        "family": "GPTNeoX",
        "path": "EleutherAI/pythia-410m",
        "sequence_length": 2048,
        "n_params": 410 * MILLION,
        "n_layers": 24,
        "fast_tokenizer": True,
    },
    "Pythia-1B": {
        "name": "Pythia-1B",
        "type": "CausalLM",
        "family": "GPTNeoX",
        "path": "EleutherAI/pythia-1b",
        "sequence_length": 2048,
        "n_params": 1 * BILLION,
        "n_layers": 16,
        "fast_tokenizer": True,
    },
    "RedPajama-3B": {
        "name": "RedPajama-3B",
        "type": "CausalLM",
        "family": "GPTNeoX",
        "path": "togethercomputer/RedPajama-INCITE-Base-3B-v1",
        "sequence_length": 2048,
        "n_params": 3 * BILLION,
        "n_layers": 32,
        "fast_tokenizer": True,
    },
    "OpenLlama-3B": {
        "name": "OpenLlama-3B",
        "type": "CausalLM",
        "family": "Llama",
        "path": "openlm-research/open_llama_3b_v2",
        "sequence_length": 2048,
        "n_params": 3 * BILLION,
        "n_layers": 26,
        "fast_tokenizer": False,
    },
    "Llama-3B-Flash": {
        "name": "Llama-3B-Flash",
        "type": "CausalLM",
        "family": "Llama",
        "path": "TaylorAI/Flash-Llama-3B",
        "sequence_length": 2048,
        "n_params": 3 * BILLION,
        "n_layers": 26,
        "fast_tokenizer": False,
    },
    "StableLM-3B": {
        "name": "StableLM-3B",
        "type": "CausalLM",
        "family": "StableLMEpoch",
        "path": "stabilityai/stablelm-3b-4e1t",
        "sequence_length": 2048,
        "n_params": 3 * BILLION,
        "n_layers": 32,
        "fast_tokenizer": True,
    },
    "XGen-7B": {
        "name": "XGen-7B",
        "type": "CausalLM",
        "family": "Llama",
        "path": "Salesforce/xgen-7b-8k-base",
        "sequence_length": 8192, # 2048 in practice
        "n_params": 7 * BILLION,
        "n_layers": 32,
        "fast_tokenizer": True,
    },
    "Falcon-7B": {
        "name": "Falcon-7B",
        "type": "CausalLM",
        "family": "Falcon",
        "path": "tiiuae/falcon-7b",
        "sequence_length": 2048,
        "n_params": 7 * BILLION,
        "n_layers": 32,
        "fast_tokenizer": True,
    },
    "Llama2-7B": {
        "name": "Llama2-7B",
        "type": "CausalLM",
        "family": "Llama",
        "path": "meta-llama/Llama-2-7b-hf",
        "sequence_length": 4096,
        "n_params": 7 * BILLION,
        "n_layers": 32,
        "fast_tokenizer": False,
    },
    "Llama2-7B-Flash": {
        "name": "Llama2-7B-Flash",
        "type": "CausalLM",
        "family": "Llama",
        "path": "TaylorAI/Flash-Llama-7B",
        "sequence_length": 32000,
        "n_params": 7 * BILLION,
        "n_layers": 32,
        "fast_tokenizer": False,
    },
    "Mistral-7B": {
        "name": "Mistral-7B",
        "type": "CausalLM",
        "family": "Mistral",
        "path": "mistralai/Mistral-7B-v0.1",
        "sequence_length": 8192,
        "n_params": 7 * BILLION,
        "n_layers": 32,
        "fast_tokenizer": True,
    },
    "Llama2-13B": {
        "name": "Llama2-13B",
        "type": "CausalLM",
        "family": "Llama",
        "path": "meta-llama/Llama-2-13b-hf",
        "sequence_length": 4096,
        "n_params": 13 * BILLION,
        "n_layers": 40,
        "fast_tokenizer": False,
    },
    "Llama2-13B-Flash": {
        "name": "Llama2-13B-Flash",
        "type": "CausalLM",
        "family": "Llama",
        "path": "TaylorAI/Flash-Llama-13B",
        "sequence_length": 4096,
        "n_params": 13 * BILLION,
        "n_layers": 40,
        "fast_tokenizer": False,
    },
    "CodeGen-1B": {
        "name": "CodeGen-1B",
        "type": "CausalLM",
        "family": "CodeGen",
        "path": "Salesforce/codegen2-1B",
        "sequence_length": 2048,
        "n_params": 1 * BILLION,
        "n_layers": 16,
        "fast_tokenizer": True,
    },
    "ReplitCode-3B": {
        "name": "ReplitCode-3B",
        "type": "CausalLM",
        "family": "MPT",
        "path": "replit/replit-code-v1-3b",
        "sequence_length": 2048,
        "n_params": 3 * BILLION,
        "n_layers": 32,
        "fast_tokenizer": False,
    },
    "CodeGen-7B": {
        "name": "CodeGen-7B",
        "type": "CausalLM",
        "family": "Llama",
        "path": "Salesforce/codegen25-7b-multi",
        "sequence_length": 2048,
        "n_params": 7 * BILLION,
        "n_layers": 32,
        "fast_tokenizer": True,
    },
    "StarCoder-15B": {
        "name": "StarCoder-15B",
        "type": "CausalLM",
        "family": "GPTBigCode",
        "path": "WizardLM/WizardCoder-15B-V1.0",
        "sequence_length": 8192, 
        "n_params": 15 * BILLION,
        "n_layers": 40,
        "fast_tokenizer": True,
    },
}

LORA_MODULES = {
    "GPTNeoX": ["query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"],
    "Llama": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],
    "StableLMEpoch": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],
    "Mistral": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "down_proj", "up_proj"],
    "CodeGen": ["qkv_proj", "out_proj", "fc_in", "fc_out"],
    "MPT": ["Wqkv", "out_proj", "up_proj", "down_proj"],
    "GPTBigCode": ["c_attn", "c_proj", "c_fc", "c_proj"],
    "Falcon": ["dense_h_to_4h", "dense_4h_to_h", "query_key_value", "dense"]
}